# regularization, 정규화
## 좋은 모델이란?
### 좋은 모델이란?
- training data를 잘 설명하는 모델 = training error를 최소화 하는 모델
<br> - Explanatory modeling
- test data에 대한 예측 성능이 좋은 모델 = expected error가 낮은 모델
<br> - Predictive modeling

- Expected error
<br> - Expected `MSE(Mean Squared Error)`
<br> - (실제 관측값 - 예측값)^2 의 평균
<br> - 조정 불가능한 에러 값(𝜎^2) + 편향^2(Bias) + 분산(Variance)

- `편향(Bias)` : 치우쳐진 정도
<br> - data 내 모든 정보를 고려하지 않음으로 인해, 지속적으로 잘못된 것들을 학습(underfitting)

- `분산(Variance)` : 퍼지는 정도
<br> - data 내 실제 현상과 관계 없는 것까지 학습하는 알고리즘의 경향을 의미(overfitting)

- ex) data가 곡선일 때,
<br> - linear한 모델이 Bias가 크다. 내가 아무리 잘 맞춰도 data 경향이 곡선이기 때문에.
<br> - overfitting된 모델이 Variance가 큼. 일반화가 낮기 때문에.
<br> <img width="500" src="https://user-images.githubusercontent.com/89369520/142297676-13c474ab-9022-4e44-9690-af87fed2b459.png">

- Least squares method(최소제곱법)
<br> - MSE를 최소화하는 회귀계수 𝛽 계산

### 정규화 모델
- 정규화, 일반화 한다는 것은 변수를 줄여서 모델을 simple하게 가져간다는 뜻.
- Loss값은 증가하지만, Variance 및 overfitting risk, 복잡도가 감소한다.
- 학습오차를 최소화하는 계수를 찾되, 회귀계수 제곱의 합이 일정수준 이하를 유지하게 한다. 변수가 많아질수록 학습데이터에 더 적합하므로 회귀계수가 커지게 되기 때문.
<br> <img width="500" src="https://user-images.githubusercontent.com/89369520/143183251-bdea4fad-362d-478d-b3eb-11f5ad6e2e84.png">
<br> - 𝜆가 크면, 회귀계수 줄이는 비중이 커서 modeling 진행이 안되며,
<br> - 𝜆가 작으면, Overfitting 위험이 있다.
- Naive한 정규화 모델
<br> 1. Rigde : 제곱오차를 최소화하면서 회귀 계수 𝛽의 제곱 제한
<br> - closed form solution 존재(미분), 변수 선택 불가능, 변수 간 상관관계가 높은 상황에서 좋은 성능을 냄, 크기가 큰 변수를 우선적으로 줄이는 경향이 있음.
<br> <img width="400" src="https://user-images.githubusercontent.com/89369520/143186558-ebb6a2bc-48d3-4260-9503-47aa71121e24.png">
<br> 2. Lasso : 제곱오차를 최소화하면서 회귀 계수 𝛽의 절대값 제한
<br> - closed form solution 미존재(절대값은 미분이 안됨), 변수 선택 가능, 변수 간 상관관계가 높은 상황에서 성능이 떨어짐
<br> <img width="400" src="https://user-images.githubusercontent.com/89369520/143186591-9a79800c-ef0f-4ff7-8c5c-263e3e45d7ba.png">
<br> 3. Elastic Net : Ridge + Lasso
<br> <img width="400" src="https://user-images.githubusercontent.com/89369520/143186702-a083feaa-6a3f-4550-a30e-350be07548a1.png">
