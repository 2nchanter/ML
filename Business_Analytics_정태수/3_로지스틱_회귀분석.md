## 로지스틱 회귀분석
- 로지스틱 회귀의 목적은 일반적인 회귀 분석의 목표와 동일하게 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용하는 것이다. 이는 독립 변수의 선형 결합으로 종속 변수를 설명한다는 관점에서는 선형 회귀 분석과 유사하다. 하지만 로지스틱 회귀는 선형 회귀 분석과는 다르게 종속 변수가 범주형 데이터를 대상으로 하며 입력 데이터가 주어졌을 때 해당 데이터의 결과가 특정 분류로 나뉘기 때문에 일종의 분류 (classification) 기법으로도 볼 수 있다.
### 배경
#### 알고리즘 종류
- Decision Tree : 의사결정나무, x, y축과 평행한 특징
- **Logistic Regression : 직선**
- Random Forest : 여러 알고리즘이 섞임
- SVM(Gaussian Kernel) : 비선형
- Neural Network : hidden node를 적게 가져가면 logistic, 직선과 비슷
>하나의 문제를 풀어내는 방식이 여러 가지 존재하기 때문에 위와 같은 알고리즘들이 존재하며, 어떤 모델도 best가 될 수 있기 때문에 어느정도의 알고리즘은 채용해봐야 함

#### Ex)
- 33명의 성인 여성에 대한 나이와 혈압 사이의 관계를 보자.
- 나이와 혈압 사이의 관계는 대략적으로 선형 관계가 존재한다고 할 수 있다.
<br><img width="300" src="https://user-images.githubusercontent.com/89369520/141037043-f33139ea-64b6-49eb-b901-18e1384b10b8.png">
- 이때 연속형 변수가 아닌 **Binary, 이진형** 변수인 Cancer Diagnosis를 사용한다면? 두 변수 사이의 관계식은 선형이라고 볼 수 없음.
<br><img width="300" src="https://user-images.githubusercontent.com/89369520/141037154-b1a99109-8da5-4c36-88d2-7fef763a9364.png">
- Q. 종속변수가 이진변수일 때 linear regression으로 X와 Y의 관계를 찾는것이 적합할까? (Linear regression : X와 Y를 **가장 잘 설명하는 함수 식**을 찾는 것, 연속형에서 미지의 X를 주었을 때, Y의 평균값을 추정값으로 보여주는 방식)
- A. No. logistic에서는 Y값이 아니라 binary인 label을 얻고자 하므로, **label을 가장 잘 구분하는 식**을 찾는쪽이 옳다.

### 알고리즘
#### Logistic f. (sigmoid f.)
- <img width="300" src="https://user-images.githubusercontent.com/89369520/141041613-63696b73-c1fb-4c8a-a04b-dd8bef7ff2e5.png">
- 목적 : 이진형 (0 or 1)dml 형태를 갖는 종속변수 분류문제에 대해 회귀식의 형태로 모형을 추정하는 것
- σ(z) = 1(1+e^(-z))
- x축 : z, 분류경계선 = b0 + b1xq + b2x2
- y축 : σ, sigmoid 함수값 = label 1로 Y가 분류될 수 있는 확률
- σ(함수값)은 label에 대한 확률이기 때문에 값이 0과 1 사이이다.
<br> - z값이 0일 때, σ(함수값)는 0.5이다. (std 0.5를 사용하지만, 조정 가능)
<br> - z가 양수일수록 분모가 작아져 σ가 큰 값이 나온다.
<br> - Y=1일 확률과 Y=0일 확률이 같은 지점, 즉, x에 데이터를 넣었을 때 z값이 0이 나오는 지점을 모으면 분류경계선, 회귀식이 된다.

#### 식 유도
- Odds, 승산
<br> - p / (1-p) = 사건이 발생할 확률 / 사건이 발생하지 않은 확률
<br> - odds 범위 : 0 ~ ∞, 비대칭성
<br> - odds는 성공확률이 높을 때 p 증가에 따라 odds 증가하는 속도가 매우 빠름
- Log odds
<br> - log odds의 범위(−∞ ~ ∞), 대칭성 확보됨
<br> - p, 성공확률이 작으면 음수, 크면 양수값을 가짐
<br><img width="300" src="https://user-images.githubusercontent.com/89369520/141058430-002b491f-3935-4ab1-967e-fe7d3e345f48.png">
- 유도
<br> - log(p/1-p) = b0 + b1x1 + ... + bpxp에서 지수함수로 변환 및 p로 표현
<br> - p = 1/1+e^-(b0 + b1x1 + ... + bpxp)

#### 어떤 모델이 더 좋은가?
- likelihood : 가능도 / y를 옳게 분류할 확률
<br> 0~1까지 값을 계속 곱하면서 작아지는 문제가 발생
- log likelihood : 곱을 합으로 변환하여 더 큰수 비교
- <img width="600" src="https://user-images.githubusercontent.com/89369520/141042916-f079605a-eabc-44c3-adb6-5209cfa7f4d0.png">
> 우도 및 로그-우도함수는 회귀계수 b에 대해 비선형이라서 선형회귀분석과 같이 명시적인 해가 존재하지 않으므로, conjugate gradient 등의 최적화 알고리즘을 차용하여 해를 구해야 함

### 결과해석
#### 예시
<img width="600" src="https://user-images.githubusercontent.com/89369520/141042582-4ecac333-6ac5-4328-9137-dfca7c11bc27.png">

#### logistic regression의 회귀계수의 의미
- linear regression
<br> - Y^ = b^0 + b^1x1 + b^2x2 + ... + b^pxp
<br> - 해당 변수가 1 증가함에 따른 종속변수의 변화량
- logistic regression
<br> - p = 1 / 1 + e^-(bo + b1x1 + b2x2 + ... + bpxp)
<br> - 해당 변수가 1 증가함에 따른 로그 승산의 변화량

#### 실제 결과데이터
- <img width="500" src="https://user-images.githubusercontent.com/89369520/141061465-b8a02679-ff6c-4c8a-9d12-6be9de5239eb.png">
- coefficient, 회귀계수
<br> - 계수가 양수 : x1+1에 따라 p가 증가 = 양의 상관관계
<br> - 계수가 음수 : x1+1에 따라 p가 감소 = 음의 상관관계
- p-value, 유의확률
<br> - 해당 변수가 통계적으로 유의미한지 여부, 0에 가까울수록, 1에서 멀수록 모델링에 중요함. 특정 유의수준 𝛼를 설정하여 해당 값 미만의 변수만을 사용하여 다시 logistic regression을 구축 가능 (보통 𝛼=0.05)
- Odds Ratio, 승산비율
<br> - 한 변수를 1만큼 증가시켰을 때 변화하는 odds의 비율
<br> = odds(x1+1 + ...) / odds(x1 + ...)
- ex) <img width="400" src="https://user-images.githubusercontent.com/89369520/141062479-48ab5072-cfc5-48d7-894a-1aa7d02855fb.png">
