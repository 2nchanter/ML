## 로지스틱 회귀분석
### 배경
- 알고리즘 종류
<br> - Decision Tree : 의사결정나무, x, y축과 평행한 특징
<br> - **Logistic Regression : 직선**
<br> - Random Forest : 여러 알고리즘이 섞임
<br> - SVM(Gaussian Kernel) : 비선형
<br> - Neural Network : hidden node를 적게 가져가면 logistic, 직선과 비슷
>하나의 문제를 풀어내는 방식이 여러 가지 존재하기 때문에 위와 같은 알고리즘들이 존재하며, 어떤 모델도 best가 될 수 있기 때문에 어느정도의 알고리즘은 채용해봐야 함

- Ex)
<br> - 33명의 성인 여성에 대한 나이와 혈압 사이의 관계를 보자.
<br> - 나이와 혈압 사이의 관계는 대략적으로 선형 관계가 존재한다고 할 수 있다.
<br> <img width="300" src="https://user-images.githubusercontent.com/89369520/141037043-f33139ea-64b6-49eb-b901-18e1384b10b8.png">
<br> - 이때 연속형 변수가 아닌 **Binary, 이진형** 변수인 Cancer Diagnosis를 사용한다면? 두 변수 사이의 관계식은 선형이라고 볼 수 없음.
<br> <img width="300" src="https://user-images.githubusercontent.com/89369520/141037154-b1a99109-8da5-4c36-88d2-7fef763a9364.png">
<br> Q. 종속변수가 이진변수일 때 linear regression으로 X와 Y의 관계를 찾는것이 적합할까? (Linear regression : X와 Y를 **가장 잘 설명하는 함수 식**을 찾는 것, 연속형에서 미지의 X를 주었을 때, Y의 평균값을 추정값으로 보여주는 방식)
<br> No. logistic에서는 Y값이 아니라 binary인 label을 얻고자 하므로, **label을 가장 잘 구분하는 식**을 찾는쪽이 옳다.

### 알고리즘
- Logistic f. (sigmoid f.)
<br> σ(z) = 1(1+e^(-z))
<br> x축 : z, 분류경계선 = b0 + b1xq + b2x2
<br> y축 : σ, sigmoid 함수값 = label 1로 Y가 분류될 수 있는 확률
<br> - σ(함수값)은 label에 대한 확률이기 때문에 값이 0과 1 사이이다.
<br> z값이 0일 때, σ(함수값)는 0.5이다.
<br> z가 양수일수록 분모가 작아져 σ가 큰 값이 나온다.
<br> Y=1일 확률과 Y=0일 확률이 같은 지점, 즉, x에 데이터를 넣었을 때 z값이 0이 나오는 지점을 모으면 분류경계선이 된다.


