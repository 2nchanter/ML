## Decision Tree
<img width="500" src="https://user-images.githubusercontent.com/89369520/143187086-59c5c881-0beb-4c0a-8370-94c69242a5ea.png">

## 의사결정나무 : Classification
### Decision Tree?
- feature들로 기준을 만들고, sample을 분류하고, 분류된 집단의 성질을 통하여 추정
- if-then 형식으로 표현되는 규칙
- 데이터 공간의 불순도가 감소하게끔 영역을 구분
<br> - 장점 : 해석력이 높으며, 직관적, 범용성이 높다. (조건을 알고 있어서 왜 이 결과가 나왔는지 알기 쉬움)
<br> - 단점 : 변동성이 높고, 샘플에 민감하다.

### Decision Tree를 이용한 분류법
- 여러 독립변수 중 하나의 독립변수를 선택하고 그 독립변수의 기준값(threshold)을 결정
- Recursive Partitioning(재귀적 분할)
<br> - 분할 후 같은 클래스에 속하는 데이터들의 비율이 높아지도록 두 그룹으로 분할.
<br> - leaf(child node가 없는 말단) node의 불순도가 0이 될 때까지 진행.
- Pruning the Tree(가지치기)
<br> - overfitting 방지 목적으로, 주변 가지를 없애서 tree를 단순화 시킴.

### impurity 지표
- Entropy
<br> <img width="500" src="https://user-images.githubusercontent.com/89369520/144145618-cd253005-3874-443a-b850-4f4e54d14ddf.png">
- Gini Index
<br> <img width="400" src="https://user-images.githubusercontent.com/89369520/144145595-4af1d996-6512-4d94-b7f7-2f506a5cfd59.png">

### IG, Information Gain
- IG = 현재 impurity - 분할 후 impurity
<br> = S Entropy - (A가중평균 x A Entropy + B가중평균 x B Entropy)
<br> 분할했을 때 Impurity 감소분에 대한 기대치

### Pruning the tree (가지치기)
- IG가 없을때까지 Recursive partitioning이 진행되면 모든 그룹에 불순도는 0이 된다.
<br> 현 상태는 training data의 noise까지 학습한 overfitting 상태이므로, test data의 Validation error가 상승한다.
<br> '비용복잡도'를 확인하며 가지치기로 일부 child node를 parent node로 통합해준다 = post-pruning
<br> <img width="300" src="https://user-images.githubusercontent.com/89369520/144146411-6343e6b1-751c-4909-a21b-23d52db035ef.png">

- Cost complexity(비용복잡도)
<br> CC = Err + a x L
<br> CC : Tree의 cost complexity, Err : test data의 Validation Err(오분류율), L : leaf node의 수
<br> = validation error가 낮을수록, model이 simple할수록 선호

- 과정
<br> <img width="500" src="https://user-images.githubusercontent.com/89369520/144148840-443c301b-e607-4e68-8470-6f94e2052fa0.png">
<br> <img width="500" src="https://user-images.githubusercontent.com/89369520/144148851-8f898480-8450-4a29-a8e1-6ea9137795d6.png">

## 의사결정나무 : Regression
### Regression tree
- leaf node는 해당 그룹 학습데이터 Y값의 평균으로 나타낸다.
- process
<br> <img width="500" src="https://user-images.githubusercontent.com/89369520/144150247-2365402d-570a-454a-b821-246347791366.png">

### CART (Classification And Regression Tree) 장단점
- 장점
<br> 사용하기 쉽고 이해하기 쉬움
<br> 해석 및 구현이 용이한 규칙 생성
- 단점
<br> 데이터가 수평 혹은 수직 분할로 구분이 어려울 때 잘 동작하지 않음
<br> 매 단계마다 "한번에 하나의 변수"를 다루기 때문에 변수 간의 상효 작용을 고려할 방법이 없음

## 분류모델 평가지표
- 분류모델이 얼마나 좋은 예측성늘을 보이는지 확인
- <img width="300" src="https://user-images.githubusercontent.com/89369520/144150534-843e2dad-e2fb-4614-ba55-90b2c88bc87f.png">
1. accuracy (정확도) TP+TN / P+N , 얼마나 정확하게 분류하는지
2. precision (정밀도) TP / TP+FP , 예측P 중에 정답이 얼마나 되는지
3. recall (재현율) TP / P [TP+FN] , 실제P 중에 정답이 얼마나 되는지
4. specificity (특이도) TN / N [TN+FP] , 실제N 중에 정답이 얼마나 되는지
5. Type 1 error (1종오류) FP / N [TN+FP] , 실제N인데 P로 선택한 비율
6. Type 2 error (2종오류) FN / P [TP+FN] , 실제P인데 N으로 선택한 비율
7. F1 Score (precision과 recall의 조화평균) 2PR / P+R
- 정밀도와 재현율이 0에 가까울 수록 F1 score도 동일하게 낮은 값을 갖도록 하기 위해 산술평균이 아닌
조화평균으로 계산. [산술평균 vs 기하평균 vs 조화평균](https://wikidocs.net/23088)
- ROC, AUC
<br> <img width="500" src="https://user-images.githubusercontent.com/89369520/144153801-e691eaa5-20de-4c06-9222-9d5a2d212831.png">
