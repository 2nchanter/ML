## Decision Tree
<img width="500" src="https://user-images.githubusercontent.com/89369520/143187086-59c5c881-0beb-4c0a-8370-94c69242a5ea.png">

## 의사결정나무
### Decision Tree?
- feature들로 기준을 만들고, sample을 분류하고, 분류된 집단의 성질을 통하여 추정
- if-then 형식으로 표현되는 규칙
- 데이터 공간의 불순도가 감소하게끔 영역을 구분
<br> - 장점 : 해석력이 높으며, 직관적, 범용성이 높다. (조건을 알고 있어서 왜 이 결과가 나왔는지 알기 쉬움)
<br> - 단점 : 변동성이 높고, 샘플에 민감하다.

### Decision Tree를 이용한 분류법
- 여러 독립변수 중 하나의 독립변수를 선택하고 그 독립변수의 기준값(threshold)을 결정
- Recursive Partitioning(재귀적 분할)
<br> - 분할 후 같은 클래스에 속하는 데이터들의 비율이 높아지도록 두 그룹으로 분할.
<br> - leaf(child node가 없는 말단) node의 불순도가 0이 될 때까지 진행.
- Pruning the Tree(가지치기)
<br> - overfitting 방지 목적으로, 주변 가지를 없애서 tree를 단순화 시킴.

### impurity 지표
- Entropy
<br> <img width="500" src="https://user-images.githubusercontent.com/89369520/144145618-cd253005-3874-443a-b850-4f4e54d14ddf.png">
- Gini Index
<br> <img width="400" src="https://user-images.githubusercontent.com/89369520/144145595-4af1d996-6512-4d94-b7f7-2f506a5cfd59.png">

### IG, Information Gain
- IG = 현재 impurity - 분할 후 impurity
<br> = S Entropy - (A가중평균 x A Entropy + B가중평균 x B Entropy)
<br> 분할했을 때 Impurity 감소분에 대한 기대치

### Pruning the tree (가지치기)
- IG가 없을때까지 Recursive partitioning이 진행되면 모든 그룹에 불순도는 0이 된다.
<br> 현 상태는 training data의 noise까지 학습한 overfitting 상태이므로, test data의 오분류율이 상승한다.
<br> '비용복잡도'를 확인하며 가지치기로 일부 child node를 parent node로 통합해준다 = post-pruning
- <img width="300" src="https://user-images.githubusercontent.com/89369520/144146411-6343e6b1-751c-4909-a21b-23d52db035ef.png">

- Cost complexity(비용복잡도)





[산술평균 vs 기하평균 vs 조화평균](https://wikidocs.net/23088)
