# 선형대수
## 1. 선형대수의 기본 요소

### 스칼라와 벡터
- scalar : 크기, 숫자 하나
- vector : 크기 + 방향 정보, 행렬

### 벡터의 차원
- 1*3 행렬 = R^3 = 3차원 (두 벡터가 같다 = 차원이 같다)

### 벡터 활용 예시
- 2차원 : 좌표
- 3차원 : 색표현
- 다차원 : 특징벡터(우리가 이제 사용할 것)

### 벡터 연산
- Transpose, 전치 연산 : xT, 행과 열을 바꾸는 연산
- p-norm, p-노름 : ||x||p, ||x||2 : 2norm이 유클리드 벡터 노름, 두 벡터간의 유클리드 거리
<br> 벡터가 얼마나 큰지(magnitude)를 알려주는 것
- inner product, 벡터의 내적 : 벡터에서 성분끼리 곱한 후 더한 값, ||u|| ||v|| cos45’
<br> 3*1 vector u, v. u . v = uTv = [ ] <- inner product or dot product

### 벡터의 종류
- zero vectors, 영벡터 : 모든 성분이 0인 벡터
- ones vectors, 일벡터 : 모든 성분이 1인 벡터
- unit vectors, u, 단위벡터 : 2norm이 1인 벡터 = 길이가 1인 벡터
- standard unit vectors, ei 표준 단위벡터, 기본 단위벡터 : 한 성분만 1인 벡터

### 행렬
- Matrix : 순서있게 배열하여 [ ]로 묶여진 것
- Row : 가로, column : 세로, element : 원소
- 차원 : 행렬의 크기, 행x열, 3x4행렬
- Ai,: : i번째 행벡터

### 행렬의 종류
- zero matrix, null matrix, 영행렬 : 모든 성분이 0인 행렬
- square matrix, 정방행렬 : 행과 열의 개수가 같은 정사각형 행렬
- diagonal matrix, 대각행렬 : 대각성분들만 있는 정방행렬, 나머지 성분 0
- identity matrix, I, 단위행렬 : 대각성분들이 1인 정방행렬, 나머지 성분 0
- triangular matrix, 삼각행렬 : 대각성분을 포함한 위쪽, 아래쪽 성분만 있는 정방행렬
- transposed matrix, AT, 전치행렬 : 주어진 행렬의 행과 열을 서로 바꾼 행렬
- 대칭행렬 : AT = A 인 정방행렬, Aij = Aji

### 선형 방정식
- linear equation, 일차 방정식 : aTx = b, n개의 미지수를 갖는 1차 방정식

### 선형 연립방정식
- system of linear equation, 선형연립방정식 : 특정 변수들에 대한 선형 방정식의 모임
- solution, Sn, 해 : xn에 sn을 대입했을 때, 각 방정식이 모두 성립하는 경우
- solution set, 해집합 : 선형시스템의 모든 해를 모아놓은 것
- equivalence, 등가 : 두 선형시스템이 같은 해집합을 갖는 경우

### 가우스 소거법
- Gauss elimination : row간 연산으로, 행렬로 구성된 방정식의 해를 구할 수 있음. 하나의 행에 상수 곱을 취한 뒤 다른 행과 더하거나 빼주는 형태의 계산법.

### 기본 행 연산
- Elementary row operation : 두 방정식의 순서 변경, 상수 곱, 더하기 같은 기본 행 연산은 해집합을 바꾸지 않는다.

### 가역행렬과 역행렬
- Square matrix A에 대하여 AB = I = BA를 만족하는 B를 A-1, inverse matrix, 역행렬 이라고 하며, A를 invertible matrix, 가역행렬, non-singular matrix, 비특이행렬 이라고 한다.
- B가 존재하지 않는다면 A를 non-invertible matrix, 비가역행렬, singular matrix, 특이행렬 이라고 한다.

### 역행렬 구하기
- 주어진 행렬 A에 I를 첨가하여 [AI]를 만든다.
- 가우스 소거법으로 A를 I로 만들어주면서 가우스행렬 [CD]를 만든다.
- C가 I, identity matrix라면, D가 A의 inverse matrix이다. 아니라면 A는 non-invertible matrix이다.
- ad-bc가 0이 아니면 A-1=1/ad-bc[d -b -c a], 0이면 A는 non-invertible matrix.
- A가 invertible matrix이면, Ax=b는 유일해 x = A-1b를 가진다. 또한 역행렬을 구해서 곱하는 것보다, 가우스소거법으로 구하는게 일반적.

### det, determinant, 행렬식
- square matrix A를 scalar값으로 mapping하는 함수 혹은 값
- matrix A의 invertible 여부 판단 (ad-bc=/0?)
- eigenvalue의 characteristic equation

## 2. 벡터공간과 부분공간
### 선형결합
- vector space : vector 덧셈과 scalar 곱에 의해 닫혀있는 벡터들의 집합
- v : vector, c : scalar.
<br> c1v1 + c2v2 + ... + cpvp : 이러한 형태를 linear combination, 선형결합 이라고 한다.
 - 선형시스템은 matrix A의 column vector의 linear combination으로 표현가능

### span
- span : 주어져있는 vector들의 모든 가능한 linear combination의 집합
<br> 해당되는 vector의 직선상에 있는 모든 점들
<br> vector가 두개 일때, v1과 v2를 지나는 평면에 있는 모든 점
- linearly dependence vector는 추가되어도 span을 증가시키지 않음

### vector equation, 해 조건
- 해가 존재하는 조건 : b ( span{a1,a2,a3} : b가 a1, a2, a3의 span에 포함
- 유일 해, trivial solution, 자명 해 : a1, a2, a3가 linearly independence, 선형독립
- 무한 해 : a1이 linearly dependence, 선형종속 일 때 (a1이 a2와 a3로 표현이 가능할 때)

## 선형변환
### linear transform
- x, y축이 A행렬에 의해서 바뀌게 되면서 새로운 좌표계를 생성한다.
- reflection, 대칭변환 : x축, y축, y=x축, 원점 반사
- contraction, 수축 과 dilation, 팽창
- rotation, 회전 과 projection, 사상

## 최소제곱법
### Least Squares Problen, 최소제곱문제(over determined system, 초과 결정)
- 하나의 데이터 당 하나의 연립방정식 나옴 100차원 중 한 점이 span 안에 들어갈 확률은 매우 적다.
<br> 해는 존재하지 않을 가능성이 높으며, 찾기도 어렵다.
- 만약 주어진 선형 연립방정식의 해가 존재하지 않는다면, 선형 연립방정식을 해결하는 것만으로는 선형 예측모형의 가중치(계수) xi값들을 구할 수 없게 됨.
- 해의 근사치를 구해야 함.

### orthogonal vectors, 직교 벡터
- u * v = 0, theta = 90', 270', cos theta = 0
- 두 vector의 inner product가 0일 때, 서로 orthogonal 하다.
- 근사해와 관련이 있다.

### over determined system
- 3개의 데이터로 해를 구해도, 4개만 되도 기존에 학습된 model로는 data를 추정할 수 없다.
- 기존의 x(해, 가중치)로 구한 b, 새로 찾아놓은 x로 구한 b와 정답 b의 차이를 보고 오차가 적은 쪽을 선택하게 된다.
- 이 때, 오차의 제곱의 합을 비교하기 때문에(부호로 인하여 오차가 적어지는 문제를 해결) 전체적으로 오차가 낮은 편이 더 좋다.

### Error Sum of Squares, 오차제곱합
- b - Ax^ ㅗ (x1a1 + x2a2 + ... + xpan) -> AT(b-Ax^)=0
- Least Squares Problem 풀이 : ATAx^ = ATb -> x^ = (ATA)^-1ATb

## 고유값과 고유벡터, 고유값 분해
### eigenvalue, eigenvector
- 행렬 A를 선형변환으로 봤을 때, 선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유벡터라 하고 이 상수배 값을 고유값이라고 함.
- Ax = 𝜆x
<br> 𝜆 : eigenvalue, 고유값, x = eigenvector, 고유벡터
- 'n차 정사각행렬의 행렬식은 고유값들의 곱과 같고, 대각합은 고유값들의 합과 같다.'

### eigendecomposition, 대각화 분해
- matrix A, 자신의 eigenvector들을 column vector로 하는 행렬과 eigenvalue를 대각원소로 하는 행렬의 곱으로 대각화 분해가 가능

### characteristic Equation, 특성방정식
- det(A-𝜆I) = 0, inverse matrix가 존재하지 않는 𝜆값을 찾는 것

### covariance matrix, 공분산행렬
- 첫번째는 구조적인 의미
<br> 각 feature들의 퍼져있음이 얼마나 유사하냐 (feature의 변동이 얼마나 닮았냐) 이다.
<br> 즉, 각 feature간의 내적을 통해 feature의 변동이 얼마나 닮았는지를 알 수 있다.
- 두번째는 수학적(기하학적) 의미
<br> 데이터를 어떻게 linear transform하고 있는가이다.
<br> 행렬은 linear tranform을 통해 다른 벡터 공간으로 mapping해주는 기능을 가지는데, 이 covariance matrix는(각자의 데이터가 서로 관련이 없는) 초기 상태에서 서로의 연관성에 대한 정보가 담겨져 있는 covariance matrix를 통해 각 데이터를 분산시켜 준다고 볼 수 있다. 이 분산시키는 형태는 마치 특정 방향으로 잡아 늘리는 (shearing) 모습을 하고 있다.
<br> (첫번째 구조적인 의미는 이미 퍼져있는 상태에서의 관계를 봤고 여기서는 데이터가 어떻게 “퍼져 나가는지”를 바라봤음)
