## 최적화와 수리계획법
### Optimization, 최적화
- 여러 실행가능(feasible = 제약식, constraints)한
<br> 대안(alternatives = 의사결정변수, decision variables)들 중에
<br> 정해진 대안평가 기준(목적식, objective function)에 근거하여
<br> 최선의 대안을 선택(목적식을 최대화, 최소화하는 의사결정변수들의 값을 결정)하는 의사결정 과정
- 특정의 집합 위에서 정의된 실수값, 함수, 정수에 대해 그 값이 최대나 최소가 되는 상태를 해석하는 문제
<br> 많은 경우 머신러닝 문제는 최적화 문제로 귀결됨

### 최적화 및 수리계획법 모형 개요
- decision variables, 의사결정 변수들
<br> 의사결정과 관련된 대안들 ex)봄학기에 학생 C의 수강과목목록
- objective function, 목적함수
<br> 의사결정변수들의 함수로 표현 ex)봄학기의 기대학점 최대화
- constraints, 제약식 : 의사결정변수들에 대한 제약들
<br> 의사결정변수들에 대한 제약들 ex)과목선택시 선수과목 이수 필수

- Mathematical Programming Models, 수리 계획법 모형
<br> 제약식들을 모두 만족하면서 목적식을 최대화, 최소화 하는 xbar의 값을 결정
<br> ex) 2차원 공간에서 linear 함수를 SSE로 찾을 때는, 모든 직선들이 대안이 되므로 제약식은 '실수여야 한다' 정도 있다. 오차제곱합이 가장 작은 답을 찾으면 정답.

## 미적분 기초
### derivative, 미분
- f'(x) = 접선의 기울기, 순간변화율, 기울기의 부호는 증가함수
- sigmoid function(?)

### partial derivative, 편미분
- 변수가 두개 이상일 경우

### Gradient, 그래디언트, ∇f
## Gradient descent
- x로부터 f의 값이 가장 가파르게 증가(steepest ascent)하는 방향
- 매 단계마다 모든 Training Set를 활용
- 벡터의 크기는 증가의 가파른 정도를 의미
```
repeat until convergence{
  𝜃𝑗:=𝜃𝑗−𝛼 ∂/∂𝜃𝑗 𝐽(𝜃0,𝜃1)
]
```
- := : "Assignment Operator (대입 연산자)
- 𝛼 : Learning Rate (학습 속도)
<br> 언덕에서 내려갈 때 얼마나 큰 걸음으로 걸을 것인가를 나타내는 양의 상수
<br> 𝛼가 클수록 step이 커짐
- ∂/∂𝜃𝑗 𝐽(𝜃0,𝜃1) : Derivative Term (미분 계수)
<br> 비용함수를 𝜃에 대해 미분한 함수

## Derivate Term, 미분계수
- 우리는 결국 𝐽(𝜃1) 이 최소가 되는 실수 𝜃1을 구해야 함. 𝜃1을 초기값으로 최소값을 찾아나갈 때, ∂/∂𝜃𝑗 𝐽(𝜃1)부분은 미분계수, 즉, x가 𝜃1일 때 함수의 기울기 임.
<br> ex) 최소값 오른쪽에 𝜃1이 위치한다고 가정.
<br> 𝛼 > 0, ∂/∂𝜃𝑗𝐽(𝜃1) > 0. 결론, 𝜃1값에서 위 값들이 빠지면서 왼쪽으로 이동하면서 반복하게 된다.

## Learning Rate, 학습 속도
- 𝛼 is too smail, gradient descent can be slow.
- 𝛼 is too large, gradient descent can overshoot the minimum.
<br> It may fail to converge, or even diverge.
> 위의 두개의 변수를 같이 사용하지 않으면, 최소값으로 수렴하지 않았음에도 제자리에서 진동하는 경우가 발생할 수 있음. 𝛼가 fix되어 있어도, 𝐽(𝜃1)가 감소하므로 최소값으로 향하게 되므로 우리로 하여금 강제적으로 𝛼를 줄이는 수고를 덜게 됨.

## 비선형 계획법 개요
### 최적해 : 극소점, 극대점
- Local maximum, Local minimum
- Global maximum, Global minimum

### Stationary Points, 안정점
- For a single variable function f(x)'s stationary point : f'(x)=0
- For a multi-variable function f(x)'s stationary point : ∇f(x)=0

### Saddle point, 안장점
- 다변수 함수에서 어느 방향에서 보면 극대점이지만, 다른방향에서 보면 극소점이 되는 점

## 수치 최적화 알고리즘



## 라그랑지 승수와 제약식 있는 최적화 







