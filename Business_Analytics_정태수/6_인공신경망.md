## 인공신경망
### Perceptron
- 입력변수의 값들의 가중합에 대한 활성함수를 적용하여 최종 결과물을 생성
<br> - 1. 입력변수의 선형결합 - 2. 선형결합 결과 값의 비선형 변환
<br> - 주어진 학습 데이터의 `입력정보 x`와 `출력정보 t`의 관계를 잘 찾도록 `가중치 w`를 잘 조정하는데 있다.
<br> <img width=500 src="https://user-images.githubusercontent.com/89369520/145024229-d624d203-c9e5-4d1e-bcbe-eb9730a29c69.png">

### Loss function
- 관계가 잘 찾아졌는지는 아래와 같은 `손실함수`로 확인한다.
<br> - regression model : squared loss
<br> - classificaion : cross-entropy loss, log loss, softmax

### Activation function
- 대표적인 활성화 함수
<br> Sigmoid : 가장 일반적으로 사용되는 activation func. [0, 1]의 범위를 가지며 학습속도가 상대적으로 느림.
<br> Tanh : 위와 형태는 유사하나 [-1, 1]의 범위를 가지며 학습속도가 상대적으로 빠름.
<br> ReLU (Rectified Linear Unit) : max(0, x). 계산이 쉽고 학습속도가 상대적으로 매우 빠름. (지수함수 형태를 사용하지 않으므로)

### Gradient
- 정답과 예측치의 차이가 클수록 가중치를 많이 업데이트 한다.
- 해당 가중치와 연결된 입력변수의 값이 클수록 가중치를 많이 업데이트 한다.

### SLP -> XOR solved?
- 하나의 직선을 그어서 XOR을 해결할 수 있나? No.
<br> linear regression이나, logistic regression을 통해 머신러닝은 결국 선이나 평면을 그리는 작업이라 이해했는데, 여기서 난관에 봉착. 실제로 과거 이 문제 때문에 인공지능은 침체기를 맞음.
<br> <img width=500 src="https://user-images.githubusercontent.com/89369520/145026682-6f051342-5052-483a-b863-ad8babb2a170.png">

### MLP (Multi-Layer Perceptron)
- 풀 수 있는 형태의 문제 여러개로 나누어 풀자.
<br> <img width=500 src="https://user-images.githubusercontent.com/89369520/145026888-4b6c1e68-64dc-43b1-9e8e-a59b154ad97c.png">
<br> <img width=500 src="https://user-images.githubusercontent.com/89369520/145027489-f21ce2c5-5f37-4dcd-83ca-16f3e8df7ca1.png">

- MLP prediction이 우수한 이유는, 선의 수와 방향에 자유도가 높기 때문이다.
<br> hidden node가 많을수록 복잡도는 증가하지만, 임의의 분류 경계면을 찾거나(분류) 굴곡이 많은 함수를 추정(회귀)할 수 있다.
<br> <img width=500 src="https://user-images.githubusercontent.com/89369520/145027110-f47bbdcc-0551-40fb-a8c1-cedb4ebe7ef6.png">

### Back-propagation
- 역전파 알고리즘, 오차의 크고 작음에 따라 가중치를 크고 작게 변화시킨다. (가중치는 특정값으로 수렴하게 된다.)

### 인공신경망의 overfitting
- 과적합을 방지하기 위한 기법
<br> 1. Epoch 수 제한
<br> 2. 복잡도 제한 (Hidden layer n node 수 제한)
<br> 3. 가중치 변화량이 일정수준 이하면 학습 종료
<br> 4. Fully-connected가 되지 않게 일부 커넥션을 잘라냄

### Summary
- 장점 : 우수한 예측성능, 복잡한 입력변수와 출력변수 관계 규명 가능
- 단점 : 하지만 인과관계까지 규명 불가능(black box model), 변수 선택 매커니즘 부재, 높은 계산복잡도

### Vanishing gradient problem
- Hidden layer가 증가함에 따라 하위 층으로 갈수록 gradient가 0이 될 가능성이 높아져 error propagation이 잘 되지 않는다. (=underfitting)
- 최근에는 activation function을 sigmoid가 아닌 ReLU를 사용하는 것으로 해결했다.
